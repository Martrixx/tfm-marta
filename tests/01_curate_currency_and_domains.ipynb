{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark():\n",
    "    return SparkSession.builder.master(\"local[2]\").getOrCreate()\n",
    "\n",
    "def test_to_usd_and_missing_rate(spark):\n",
    "    exch = spark.createDataFrame([(\"EUR\", 1.1), (\"USD\", 1.0)], \"Currency string, AvgRate double\")\n",
    "    df = spark.createDataFrame([\n",
    "        (\"P1\",\"EUR\",100.0), (\"P2\",\"USD\",50.0), (\"P3\",\"GBP\",10.0)\n",
    "    ], \"ProjectID string, Currency string, Price double\")\n",
    "\n",
    "    d = {r[\"Currency\"]: r[\"AvgRate\"] for r in exch.collect()}\n",
    "    from pyspark.sql.functions import udf, lit\n",
    "    from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "    @udf(DoubleType())\n",
    "    def to_usd(price, currency):\n",
    "        if currency is None or price is None: return None\n",
    "        rate = d.get(currency)\n",
    "        return price*rate if rate is not None else None\n",
    "\n",
    "    @udf(StringType())\n",
    "    def usd_flag(currency):\n",
    "        return \"OK\" if currency in d else \"MISSING_RATE\"\n",
    "\n",
    "    out = df.withColumn(\"Price_USD\", to_usd(\"Price\",\"Currency\")) \\\n",
    "            .withColumn(\"USD_Rate_Status\", usd_flag(\"Currency\"))\n",
    "\n",
    "    rows = {r[\"ProjectID\"]: (r[\"Price_USD\"], r[\"USD_Rate_Status\"]) for r in out.collect()}\n",
    "    assert rows[\"P1\"] == (110.0, \"OK\")\n",
    "    assert rows[\"P2\"] == (50.0, \"OK\")\n",
    "    assert rows[\"P3\"][0] is None and rows[\"P3\"][1] == \"MISSING_RATE\"\n",
    "\n",
    "def test_integrity_flags_written(spark):\n",
    "    cons = spark.createDataFrame([\n",
    "        (\"A\",\"Before_RtB\",\"Cat1\",None),\n",
    "        (\"A\",\"After_RtB\",\"Cat1\",1000.0),\n",
    "        (\"B\",\"After_RtB\",\"CatX\",500.0),     \n",
    "        (\"C\",\"Before_RtB\",\"Cat1\",None),\n",
    "        (\"C\",\"Before_RtB\",\"Cat2\",None),     \n",
    "        (\"D\",\"After_RtB\",\"Cat1\",100.0),\n",
    "        (\"D\",\"After_RtB\",\"Cat1\",200.0),     \n",
    "    ], \"Project ID string, Project phase string, Material Category string, RtB Budget double\")\n",
    "\n",
    "    import pyspark.sql.functions as F\n",
    "    issues = []\n",
    "\n",
    "    phases = cons.select(\"Project ID\",\"Project phase\").dropDuplicates()\n",
    "    proj_phase = phases.groupBy(\"Project ID\").agg(F.collect_set(\"Project phase\").alias(\"phases\"))\n",
    "    after_without_before = proj_phase.where(F.array_contains(\"phases\",\"After_RtB\") & ~F.array_contains(\"phases\",\"Before_RtB\"))\n",
    "    if after_without_before.count()>0:\n",
    "        issues.append(after_without_before.withColumn(\"issue\", F.lit(\"PHASE_AFTER_WITHOUT_BEFORE\")))\n",
    "\n",
    "    matcat = cons.select(\"Project ID\",\"Material Category\").dropDuplicates()\n",
    "    dup_matcat = matcat.groupBy(\"Project ID\").agg(F.countDistinct(\"Material Category\").alias(\"n\")).where(\"n>1\")\n",
    "    if dup_matcat.count()>0:\n",
    "        issues.append(dup_matcat.withColumn(\"issue\", F.lit(\"MULTIPLE_MATCAT_PER_PROJECT\")))\n",
    "\n",
    "    budget = (cons.where((F.col(\"Project phase\")==\"After_RtB\") & F.col(\"RtB Budget\").isNotNull())\n",
    "                   .select(\"Project ID\",\"RtB Budget\").dropDuplicates())\n",
    "    dup_budget = budget.groupBy(\"Project ID\").agg(F.countDistinct(\"RtB Budget\").alias(\"n\")).where(\"n>1\")\n",
    "    if dup_budget.count()>0:\n",
    "        issues.append(dup_budget.withColumn(\"issue\", F.lit(\"MULTIPLE_RTB_BUDGET_PER_PROJECT\")))\n",
    "\n",
    "    assert len(issues) == 3  # los tres chequeos encontraron algo\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
