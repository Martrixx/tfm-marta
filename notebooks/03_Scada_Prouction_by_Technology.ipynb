{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c24bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG = \"\"  \n",
    "DB_STG = \"stg_scada\"\n",
    "DB_REF = \"ref\"\n",
    "DB_CUR = \"cur\"\n",
    "DB_FACT5 = \"fact5m\"\n",
    "DB_MART = \"mart\"\n",
    "DB_QA = \"qa\"\n",
    "DB_DIM = \"dim\"\n",
    "\n",
    "TBL_READ = f\"{DB_STG}.readings_5m\"\n",
    "TBL_ASSET = f\"{DB_STG}.asset_master\"\n",
    "TBL_PC   = f\"{DB_REF}.power_curve_wind\"     \n",
    "TBL_POA  = f\"{DB_REF}.irradiance_ref\"       \n",
    "TBL_SET  = f\"{DB_REF}.dispatch_limits\"       \n",
    "TBL_DT   = f\"{DB_REF}.downtimes\"             \n",
    "\n",
    "def qt(t): return f\"{CATALOG}.{t}\" if CATALOG else t\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DB_FACT5}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DB_MART}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DB_QA}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DB_DIM}\")\n",
    "\n",
    "from pyspark.sql import functions as F, Window as W\n",
    "from pyspark.sql.types import TimestampType, DoubleType, IntegerType\n",
    "\n",
    "# -------------------------------\n",
    "# 0) Parámetros de señales \n",
    "# -------------------------------\n",
    "SIG = {\n",
    "    \"power_kw\": [\"ActivePower\", \"AC_Power_kW\", \"WTG_ActivePower\"],\n",
    "    \"energy_kwh\": [\"Energy_kWh\", \"AC_Energy_kWh\"],   \n",
    "    \"availability_pct\": [\"Availability\", \"TechAvailability\"],\n",
    "    \"wind_speed_ms\": [\"WindSpeed_ms\", \"NacelleWindSpeed\"],\n",
    "    \"poa_wm2\": [\"POA_Wm2\"],                 \n",
    "    \"temp_c\": [\"TempC\", \"ModuleTemp_C\"],\n",
    "    \"status\": [\"Status\", \"OperatingState\"],\n",
    "    \"reactive_kvar\": [\"ReactivePower_kVAr\"],\n",
    "    \"freq_hz\": [\"Frequency_Hz\"],\n",
    "    \"voltage_kv\": [\"Voltage_kV\"]\n",
    "}\n",
    "FIVE_MIN_HOURS = 5.0/60.0  \n",
    "\n",
    "# -------------------------------\n",
    "# 1) Lecturas base  y maestro\n",
    "# -------------------------------\n",
    "df = (spark.table(qt(TBL_READ))\n",
    "      .withColumn(\"ts_utc\", F.col(\"ts_utc\").cast(TimestampType()))\n",
    "      .withColumn(\"value\", F.col(\"value\").cast(DoubleType()))\n",
    "      .withColumn(\"signal_lc\", F.lower(F.col(\"signal\"))))\n",
    "\n",
    "assets = spark.table(qt(TBL_ASSET)) \\\n",
    "              .withColumnRenamed(\"AssetId\",\"AssetId\") \\\n",
    "              .withColumn(\"RatedPower_kW\", F.col(\"RatedPower_kW\").cast(DoubleType()))\n",
    "\n",
    "# helper: filtrar por grupo de señales\n",
    "def pick(df_in, names):\n",
    "    names_lc = [n.lower() for n in names]\n",
    "    return df_in.where(F.col(\"signal_lc\").isin(names_lc))\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Reamostrado / completitud a 5-min \n",
    "# -------------------------------\n",
    "min_ts, max_ts = df.agg(F.min(\"ts_utc\"), F.max(\"ts_utc\")).first()\n",
    "if not min_ts or not max_ts:\n",
    "    raise ValueError(\"No hay datos en stg_scada.readings_5m\")\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "def daterange(start, end, step_minutes=5):\n",
    "    while start <= end:\n",
    "        yield start\n",
    "        start += timedelta(minutes=step_minutes)\n",
    "\n",
    "\n",
    "asset_ids = [r[\"asset_id\"] for r in df.select(\"asset_id\").distinct().collect()]\n",
    "grid_rows = []\n",
    "for a in asset_ids:\n",
    "    t = min_ts\n",
    "    while t <= max_ts:\n",
    "        grid_rows.append((a, t))\n",
    "        t += timedelta(minutes=5)\n",
    "grid_schema = \"asset_id string, ts_utc timestamp\"\n",
    "grid = spark.createDataFrame(grid_rows, grid_schema)\n",
    "\n",
    "\n",
    "pwr = (pick(df, SIG[\"power_kw\"])\n",
    "       .groupBy(\"asset_id\",\"ts_utc\")\n",
    "       .agg(F.avg(\"value\").alias(\"P_kW\")))\n",
    "\n",
    "base = grid.join(pwr, [\"asset_id\",\"ts_utc\"], \"left\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Calidad y *gap handling* ligero\n",
    "# -------------------------------\n",
    "# Quality flags:\n",
    "#   - missing: sin dato\n",
    "#   - zero_flat: rachas de ceros anómalos durante producción (no de noche, en solar)\n",
    "#   - outlier: percentil 99.9\n",
    "#   - clipped: igual al límite de despacho \n",
    "#   - maintenance: dentro de ventana de downtime\n",
    "\n",
    "# a) Missing\n",
    "base = base.withColumn(\"q_missing\", F.when(F.col(\"P_kW\").isNull(), F.lit(1)).otherwise(F.lit(0)))\n",
    "\n",
    "# b) Outliers (por asset)\n",
    "w_asset = W.partitionBy(\"asset_id\")\n",
    "p999 = base.approxQuantile(\"P_kW\", [0.999], 0.01)[0] if base.filter(\"P_kW is not null\").count()>0 else None\n",
    "base = base.withColumn(\"q_outlier\", F.when((F.col(\"P_kW\") > F.lit(p999)) & F.col(\"P_kW\").isNotNull(), 1).otherwise(0))\n",
    "\n",
    "# c) Límite de despacho \n",
    "if spark.catalog.tableExists(qt(TBL_SET)):\n",
    "    setp = spark.table(qt(TBL_SET)).select(\"asset_id\",\"ts_utc\", F.col(\"Limit_kW\").cast(DoubleType()))\n",
    "    base = base.join(setp, [\"asset_id\",\"ts_utc\"], \"left\")\n",
    "    base = base.withColumn(\"q_clipped\", F.when((F.col(\"Limit_kW\").isNotNull()) & (F.abs(F.col(\"P_kW\")-F.col(\"Limit_kW\")) < 1e-6), 1).otherwise(0))\n",
    "else:\n",
    "    base = base.withColumn(\"Limit_kW\", F.lit(None).cast(DoubleType())).withColumn(\"q_clipped\", F.lit(0))\n",
    "\n",
    "# d) Mantenimiento \n",
    "if spark.catalog.tableExists(qt(TBL_DT)):\n",
    "    dt = spark.table(qt(TBL_DT)).select(\"asset_id\",\"start_utc\",\"end_utc\")\n",
    "\n",
    "    base = (base.join(dt, \"asset_id\", \"left\")\n",
    "                 .withColumn(\"q_maint\",\n",
    "                             F.when((F.col(\"start_utc\").isNotNull()) &\n",
    "                                    (F.col(\"ts_utc\")>=F.col(\"start_utc\")) &\n",
    "                                    (F.col(\"ts_utc\")<=F.col(\"end_utc\")), 1).otherwise(0))\n",
    "                 .drop(\"start_utc\",\"end_utc\"))\n",
    "else:\n",
    "    base = base.withColumn(\"q_maint\", F.lit(0))\n",
    "\n",
    "\n",
    "w_ff = W.partitionBy(\"asset_id\").orderBy(\"ts_utc\").rowsBetween(-2, 0)\n",
    "base = base.withColumn(\"P_kW_ff\", F.first(\"P_kW\", ignorenulls=True).over(w_ff))\n",
    "base = base.withColumn(\"P_kW_clean\", F.when(F.col(\"q_missing\")==1, F.col(\"P_kW_ff\")).otherwise(F.col(\"P_kW\")))\n",
    "base = base.withColumn(\"P_kW_clean\", F.when(F.col(\"q_outlier\")==1, None).otherwise(F.col(\"P_kW_clean\")))\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Señales adicionales para KPIs\n",
    "# -------------------------------\n",
    "avail = (pick(df, SIG[\"availability_pct\"])\n",
    "         .groupBy(\"asset_id\",\"ts_utc\")\n",
    "         .agg(F.avg(\"value\").alias(\"Availability_%\")))\n",
    "\n",
    "wind = (pick(df, SIG[\"wind_speed_ms\"])\n",
    "        .groupBy(\"asset_id\",\"ts_utc\")\n",
    "        .agg(F.avg(\"value\").alias(\"WindSpeed_ms\")))\n",
    "\n",
    "status = (pick(df, SIG[\"status\"])\n",
    "          .groupBy(\"asset_id\",\"ts_utc\")\n",
    "          .agg(F.first(\"value\", ignorenulls=True).alias(\"StatusCode\")))\n",
    "\n",
    "# Irradiancia / temperatura \n",
    "if any(spark.catalog.tableExists(qt(t)) for t in [TBL_POA]):\n",
    "    poa = spark.table(qt(TBL_POA))\n",
    "    poa_sel = poa.select(\"asset_id\",\"ts_utc\",\n",
    "                         F.col(\"POA_Wm2\").cast(DoubleType()).alias(\"POA_Wm2\"),\n",
    "                         F.col(\"TempC\").cast(DoubleType()).alias(\"TempC\"))\n",
    "else:\n",
    "    poa_sel = (pick(df, SIG[\"poa_wm2\"])\n",
    "               .groupBy(\"asset_id\",\"ts_utc\")\n",
    "               .agg(F.avg(\"value\").alias(\"POA_Wm2\")))\n",
    "    # temp opcional\n",
    "    temp = (pick(df, SIG[\"temp_c\"])\n",
    "            .groupBy(\"asset_id\",\"ts_utc\")\n",
    "            .agg(F.avg(\"value\").alias(\"TempC\")))\n",
    "    poa_sel = poa_sel.join(temp, [\"asset_id\",\"ts_utc\"], \"left\")\n",
    "\n",
    "# Unir señales\n",
    "wide = (base.join(avail, [\"asset_id\",\"ts_utc\"], \"left\")\n",
    "             .join(wind, [\"asset_id\",\"ts_utc\"], \"left\")\n",
    "             .join(poa_sel, [\"asset_id\",\"ts_utc\"], \"left\")\n",
    "             .join(status, [\"asset_id\",\"ts_utc\"], \"left\"))\n",
    "\n",
    "wide = wide.withColumn(\"E_kWh_5m\", F.when(F.col(\"P_kW_clean\").isNotNull(), F.col(\"P_kW_clean\")*FIVE_MIN_HOURS))\n",
    "\n",
    "# -------------------------------\n",
    "# 5) KPI por tecnología\n",
    "# -------------------------------\n",
    "cur_asset = assets.select(\"AssetId\",\"AssetType\",\"PlantId\",\"RatedPower_kW\")\\\n",
    "                  .withColumnRenamed(\"AssetId\",\"asset_id\")\n",
    "\n",
    "wide = wide.join(cur_asset, \"asset_id\", \"left\")\n",
    "\n",
    "# a) Capacity Factor (CF) = Pavg / RatedPower\n",
    "wide = wide.withColumn(\"CF\",\n",
    "                       F.when(F.col(\"RatedPower_kW\")>0,\n",
    "                              F.col(\"P_kW_clean\")/F.col(\"RatedPower_kW\")).otherwise(None))\n",
    "\n",
    "# b) Curtailment kW (si hay Limit_kW)\n",
    "wide = wide.withColumn(\"Curtailment_kW\",\n",
    "                       F.when((F.col(\"Limit_kW\").isNotNull()) & (F.col(\"P_kW_clean\").isNotNull()),\n",
    "                              F.greatest(F.col(\"Limit_kW\")-F.col(\"P_kW_clean\"), F.lit(0.0))).otherwise(None))\n",
    "\n",
    "# c) Performance Ratio (PR) — Solar:\n",
    "#    PR_5m = E_meas / (POA_Wm2 * Area * eta_ref * 5min) → simplificamos usando RatedPower como proxy:\n",
    "#    PR_5m ~= P_meas / (POA_Wm2/1000 * RatedPower_kW)\n",
    "#    (si POA_Wm2 es None, queda null)\n",
    "wide = wide.withColumn(\"PR\",\n",
    "                       F.when((F.col(\"AssetType\").isin(\"PV\",\"Solar\")) &\n",
    "                              (F.col(\"POA_Wm2\").isNotNull()) &\n",
    "                              (F.col(\"RatedPower_kW\")>0),\n",
    "                              F.col(\"P_kW_clean\") / (F.col(\"POA_Wm2\")/1000.0 * F.col(\"RatedPower_kW\")))\n",
    "                        .otherwise(None))\n",
    "\n",
    "# d) Wind Performance Index (WPI)\n",
    "if spark.catalog.tableExists(qt(TBL_PC)):\n",
    "    pc = spark.table(qt(TBL_PC)).select(\n",
    "        F.col(\"WindSpeed\").cast(DoubleType()).alias(\"WindSpeed_ms\"),\n",
    "        F.col(\"ExpectedPower_kW\").cast(DoubleType()).alias(\"Pexp_kW\"),\n",
    "        \"AssetTypeCanonical\"\n",
    "    ).where(F.col(\"AssetTypeCanonical\").isin(\"Wind\"))\n",
    "    # para unir por bin de velocidad, discretizamos\n",
    "    wide = wide.withColumn(\"ws_bin\", F.round(F.col(\"WindSpeed_ms\"), 0))\n",
    "    pc_bin = pc.withColumn(\"ws_bin\", F.round(F.col(\"WindSpeed_ms\"), 0)).drop(\"WindSpeed_ms\")\n",
    "    wide = (wide.where(F.col(\"AssetType\").isin(\"WTG\",\"Wind\"))\n",
    "                 .join(pc_bin.drop(\"AssetTypeCanonical\"), [\"ws_bin\"], \"left\")\n",
    "                 .withColumn(\"WPI\", F.when(F.col(\"Pexp_kW\")>0, F.col(\"P_kW_clean\")/F.col(\"Pexp_kW\")).otherwise(None))\n",
    "                 .drop(\"ws_bin\",\"Pexp_kW\"))\n",
    "else:\n",
    "    wide = wide.withColumn(\"WPI\", F.lit(None).cast(DoubleType()))\n",
    "\n",
    "# e) Technical Availability ajustada \n",
    "wide = wide.withColumn(\"AvailabilityAdj_%\",\n",
    "                       F.when(F.col(\"q_maint\")==1, None).otherwise(F.col(\"Availability_%\")))\n",
    "\n",
    "# -------------------------------\n",
    "# 6) Persistir FACT \n",
    "# -------------------------------\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {qt(DB_FACT5+'.ElectricalEnergyGeneration_5m')}\")\n",
    "(wide.select(\n",
    "    \"asset_id\",\"PlantId\",\"ts_utc\",\"AssetType\",\"RatedPower_kW\",\n",
    "    \"P_kW_clean\",\"E_kWh_5m\",\"Availability_%\",\"AvailabilityAdj_%\",\"WindSpeed_ms\",\n",
    "    \"POA_Wm2\",\"TempC\",\"Limit_kW\",\"Curtailment_kW\",\"PR\",\"CF\",\"WPI\",\"StatusCode\",\n",
    "    \"q_missing\",\"q_outlier\",\"q_clipped\",\"q_maint\"\n",
    " ).write.mode(\"overwrite\").saveAsTable(qt(f\"{DB_FACT5}.ElectricalEnergyGeneration_5m\")))\n",
    "\n",
    "def save_tech(df_in, tech_list, name):\n",
    "    sub = df_in.where(F.col(\"AssetType\").isin(*tech_list))\n",
    "    if sub.limit(1).count()>0:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {qt(DB_FACT5+'.'+name)}\")\n",
    "        sub.write.mode(\"overwrite\").saveAsTable(qt(f\"{DB_FACT5}.{name}\"))\n",
    "\n",
    "save_tech(wide, [\"WTG\",\"Wind\"], \"WindGeneration_5m\")\n",
    "save_tech(wide, [\"PV\",\"Solar\"], \"SolarGeneration_5m\")\n",
    "save_tech(wide, [\"TRANS\",\"Transmission\"], \"Transmission_5m\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7) MART de KPIs\n",
    "# -------------------------------\n",
    "# 5-min \n",
    "kpi_cols = [\"asset_id\",\"PlantId\",\"ts_utc\",\"P_kW_clean\",\"E_kWh_5m\",\"CF\",\"PR\",\"WPI\",\n",
    "            \"Availability_%\",\"AvailabilityAdj_%\",\"Curtailment_kW\",\"Limit_kW\",\n",
    "            \"WindSpeed_ms\",\"POA_Wm2\",\"TempC\",\"StatusCode\",\n",
    "            \"q_missing\",\"q_outlier\",\"q_clipped\",\"q_maint\"]\n",
    "kpi5 = wide.select(*kpi_cols)\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {qt(DB_MART+'.KPI_5m')}\")\n",
    "kpi5.write.mode(\"overwrite\").saveAsTable(qt(f\"{DB_MART}.KPI_5m\"))\n",
    "\n",
    "# Dim DateId\n",
    "dim_date = (spark.table(qt(f\"{DB_DIM}.Date\"))\n",
    "            if spark.catalog.tableExists(qt(f\"{DB_DIM}.Date\"))\n",
    "            else None)\n",
    "if dim_date is None:\n",
    "    from pyspark.sql.types import DateType\n",
    "    dim_date = (kpi5\n",
    "                .select(F.to_date(\"ts_utc\").alias(\"Date\"))\n",
    "                .distinct()\n",
    "                .withColumn(\"DateId\", F.date_format(\"Date\",\"yyyyMMdd\").cast(IntegerType())))\n",
    "else:\n",
    "    dim_date = dim_date.select(\"Date\",\"DateId\")\n",
    "\n",
    "kpi5 = kpi5.withColumn(\"Date\", F.to_date(\"ts_utc\")).join(dim_date, \"Date\", \"left\")\n",
    "\n",
    "# Agregación ponderada \n",
    "agg_daily = (kpi5.groupBy(\"asset_id\",\"PlantId\",\"DateId\")\n",
    "             .agg(F.sum(\"E_kWh_5m\").alias(\"Energy_kWh\"),\n",
    "                  F.avg(\"P_kW_clean\").alias(\"Pavg_kW\"),\n",
    "                  F.max(\"P_kW_clean\").alias(\"Pmax_kW\"),\n",
    "                  F.avg(\"CF\").alias(\"CF_avg\"),\n",
    "                  F.avg(\"PR\").alias(\"PR_avg\"),\n",
    "                  F.avg(\"WPI\").alias(\"WPI_avg\"),\n",
    "                  F.avg(\"Availability_%\").alias(\"Availability_avg\"),\n",
    "                  F.avg(\"AvailabilityAdj_%\").alias(\"AvailabilityAdj_avg\"),\n",
    "                  F.sum(\"Curtailment_kW\").alias(\"Curtailment_kW_sum\")))\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {qt(DB_MART+'.KPI_Daily')}\")\n",
    "agg_daily.write.mode(\"overwrite\").saveAsTable(qt(f\"{DB_MART}.KPI_Daily\"))\n",
    "\n",
    "# Mensual\n",
    "kpi5m = kpi5.withColumn(\"YearMonth\", F.date_format(\"Date\",\"yyyyMM\").cast(IntegerType()))\n",
    "agg_month = (kpi5m.groupBy(\"asset_id\",\"PlantId\",\"YearMonth\")\n",
    "             .agg(F.sum(\"E_kWh_5m\").alias(\"Energy_kWh\"),\n",
    "                  F.avg(\"P_kW_clean\").alias(\"Pavg_kW\"),\n",
    "                  F.max(\"P_kW_clean\").alias(\"Pmax_kW\"),\n",
    "                  F.avg(\"CF\").alias(\"CF_avg\"),\n",
    "                  F.avg(\"PR\").alias(\"PR_avg\"),\n",
    "                  F.avg(\"WPI\").alias(\"WPI_avg\"),\n",
    "                  F.avg(\"Availability_%\").alias(\"Availability_avg\"),\n",
    "                  F.avg(\"AvailabilityAdj_%\").alias(\"AvailabilityAdj_avg\"),\n",
    "                  F.sum(\"Curtailment_kW\").alias(\"Curtailment_kW_sum\")))\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {qt(DB_MART+'.KPI_Monthly')}\")\n",
    "agg_month.write.mode(\"overwrite\").saveAsTable(qt(f\"{DB_MART}.KPI_Monthly\"))\n",
    "\n",
    "# -------------------------------\n",
    "# 8) QA de calidad (porcentaje de puntos afectados)\n",
    "# -------------------------------\n",
    "qa = (wide.groupBy(\"asset_id\")\n",
    "      .agg(F.avg(F.col(\"q_missing\").cast(\"double\")).alias(\"pct_missing\"),\n",
    "           F.avg(F.col(\"q_outlier\").cast(\"double\")).alias(\"pct_outlier\"),\n",
    "           F.avg(F.col(\"q_clipped\").cast(\"double\")).alias(\"pct_clipped\"),\n",
    "           F.avg(F.col(\"q_maint\").cast(\"double\")).alias(\"pct_maint\")))\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {qt(DB_QA+'.KPI_Quality_5m')}\")\n",
    "qa.write.mode(\"overwrite\").saveAsTable(qt(f\"{DB_QA}.KPI_Quality_5m\"))\n",
    "\n",
    "print(\" FACT 5-min y MART de KPIs generados.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
